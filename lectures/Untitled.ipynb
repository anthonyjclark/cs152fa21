{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3eddbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8564fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (\"The dog ate the apple\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Everybody read that book\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Trapp is sleeping\".lower().split(), [\"N\", \"V\", \"V\"]),\n",
    "    (\"Everybody ate the apple\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Cats are good\".lower().split(), [\"N\", \"V\", \"D\"]),\n",
    "    (\"Dogs are not as good as cats\".lower().split(),[\"N\", \"V\", \"D\", \"D\", \"D\", \"D\", \"N\"]),\n",
    "    (\"Dogs eat dog food\".lower().split(), [\"N\", \"V\", \"N\", \"N\"]),\n",
    "    (\"Watermelon is the best food\".lower().split(), [\"N\", \"V\", \"D\", \"D\", \"N\"]),\n",
    "    (\"I want a milkshake right now\".lower().split(), [\"N\", \"V\", \"D\", \"N\", \"D\", \"D\"]),\n",
    "    (\"I have too much homework\".lower().split(), [\"N\", \"V\", \"D\", \"D\", \"N\"]),\n",
    "    (\"Zoom won't work\".lower().split(), [\"N\", \"D\", \"V\"]),\n",
    "    (\"Pie also sounds good\".lower().split(), [\"N\", \"D\", \"V\", \"D\"]),\n",
    "    (\"The college is having the department fair this Friday\".lower().split(),[\"D\", \"N\", \"V\", \"V\", \"D\", \"N\", \"N\", \"D\", \"N\"]),\n",
    "    (\"Research interests span many areas\".lower().split(), [\"N\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Alex is finishing his Ph.D\".lower().split(), [\"N\", \"V\", \"V\", \"D\", \"N\"]),\n",
    "    (\"She is the author\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"It is almost the end of the semester\".lower().split(), [\"N\", \"V\", \"D\", \"D\", \"N\", \"D\", \"D\", \"N\"]),\n",
    "    (\"Blue is a color\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"They wrote a book\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The syrup covers the pancake\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Harrison has these teeth\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The numbers are fractions\".lower().split(), [\"D\", \"N\", \"V\", \"N\"]),\n",
    "    (\"Yesterday happened\".lower().split(), [\"N\", \"V\"]),\n",
    "    (\"Caramel is sweet\".lower().split(), [\"N\", \"V\", \"D\"]),\n",
    "    (\"Computers use electricity\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"Gold is a valuable thing\".lower().split(), [\"N\", \"V\", \"D\", \"D\", \"N\"]),\n",
    "    (\"This extension cord helps\".lower().split(), [\"D\", \"D\", \"N\", \"V\"]),\n",
    "    (\"It works on my machine\".lower().split(), [\"N\", \"V\", \"D\", \"D\", \"N\"]),\n",
    "    (\"We have the words\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Trapp is a dog\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"This is a computer\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I love lamps\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"I walked outside\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"You never bike home\".lower().split(), [\"N\", \"D\", \"V\", \"N\"]),\n",
    "    (\"You are a wizard Harry\".lower().split(), [\"N\", \"V\", \"D\", \"N\", \"N\"]),\n",
    "    (\"Trapp ate the shoe\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Jett failed his test\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Alice won the game\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The class lasted a semester\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The tree had a branch\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I ran a race\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The dog barked\".lower().split(), [\"D\", \"N\", \"V\"]),\n",
    "    (\"Toby hit the wall\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Zayn ate an apple\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The cat fought the dog\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I got an A\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The A hurt\".lower().split(), [\"D\", \"N\", \"V\"]),\n",
    "    (\"I jump\".lower().split(), [\"N\", \"V\"]),\n",
    "    (\"I drank a yerb\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The snake ate a fruit\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I played the game\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I watched a movie\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Clark fixed the audio\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I went to Frary\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I go to Pomona\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Food are friends not fish\".lower().split(), [\"N\", \"V\", \"N\", \"D\", \"N\"]),\n",
    "    (\"You are reading this\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Wonderland protocol is amazing\".lower().split(), [\"D\", \"N\", \"V\", \"D\"]),\n",
    "    (\"This is a sentence\".lower().split(), [\"D\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I should be doing homework\".lower().split(), [\"N\", \"V\", \"V\", \"V\", \"N\"]),\n",
    "    (\"Computers are tools\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"The whale swims\".lower().split(), [\"D\", \"N\", \"V\"]),\n",
    "    (\"A cup is filled\".lower().split(), [\"D\", \"N\", \"V\", \"V\"]),\n",
    "    (\"This is a cat\".lower().split(), [\"D\", \"V\", \"D\", \"N\"]),\n",
    "    (\"These are trees\".lower().split(), [\"D\", \"V\", \"N\"]),\n",
    "    (\"The cat is the teacher\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I ate food today\".lower().split(), [\"N\", \"V\", \"N\", \"N\"]),\n",
    "    (\"I am a human\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The cat sleeps\".lower().split(), [\"D\", \"N\", \"V\"]),\n",
    "    (\"Whales are mammals\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"I like turtles\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"A shark ate me\".lower().split(), [\"D\", \"N\", \"V\", \"N\"]),\n",
    "]\n",
    "\n",
    "# There are mirrors\tD, V, N\n",
    "# The bus spins\tD, N, V\n",
    "# Computers are machines\tN, V, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0587d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "total_words = 0\n",
    "\n",
    "tag_to_index = {}\n",
    "tag_list = []\n",
    "total_tags = 0\n",
    "\n",
    "for sentence, tags in dataset:\n",
    "    assert len(sentence) == len(tags)\n",
    "    total_words += len(sentence)\n",
    "    for word in sentence:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "\n",
    "    total_tags += len(tags)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_index:\n",
    "            tag_to_index[tag] = len(tag_to_index)\n",
    "            tag_list.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eaa192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Vocabulary Indices\n",
      "-------------------------------\n",
      "             a => 23\n",
      "          alex => 48\n",
      "         alice => 106\n",
      "        almost => 55\n",
      "          also => 35\n",
      "            am => 160\n",
      "       amazing => 147\n",
      "            an => 121\n",
      "         apple =>  3\n",
      "           are => 12\n",
      "         areas => 47\n",
      "            as => 16\n",
      "           ate =>  2\n",
      "         audio => 136\n",
      "        author => 53\n",
      "        barked => 116\n",
      "            be => 150\n",
      "          best => 20\n",
      "          bike => 98\n",
      "          blue => 59\n",
      "          book =>  7\n",
      "        branch => 113\n",
      "       caramel => 74\n",
      "           cat => 122\n",
      "          cats => 11\n",
      "         clark => 134\n",
      "         class => 109\n",
      "       college => 37\n",
      "         color => 60\n",
      "      computer => 91\n",
      "     computers => 76\n",
      "          cord => 83\n",
      "        covers => 64\n",
      "           cup => 155\n",
      "    department => 39\n",
      "           dog =>  1\n",
      "          dogs => 14\n",
      "         doing => 151\n",
      "         drank => 127\n",
      "           eat => 17\n",
      "   electricity => 78\n",
      "           end => 56\n",
      "     everybody =>  4\n",
      "     extension => 82\n",
      "        failed => 104\n",
      "          fair => 40\n",
      "        filled => 156\n",
      "     finishing => 49\n",
      "          fish => 143\n",
      "         fixed => 135\n",
      "          food => 18\n",
      "        fought => 123\n",
      "     fractions => 71\n",
      "         frary => 139\n",
      "        friday => 42\n",
      "       friends => 142\n",
      "         fruit => 130\n",
      "          game => 108\n",
      "            go => 140\n",
      "          gold => 79\n",
      "          good => 13\n",
      "           got => 124\n",
      "           had => 112\n",
      "      happened => 73\n",
      "      harrison => 66\n",
      "         harry => 101\n",
      "           has => 67\n",
      "          have => 27\n",
      "        having => 38\n",
      "         helps => 84\n",
      "           his => 50\n",
      "           hit => 118\n",
      "          home => 99\n",
      "      homework => 30\n",
      "         human => 161\n",
      "          hurt => 125\n",
      "             i => 21\n",
      "     interests => 44\n",
      "            is =>  9\n",
      "            it => 54\n",
      "          jett => 103\n",
      "          jump => 126\n",
      "         lamps => 93\n",
      "        lasted => 110\n",
      "          like => 165\n",
      "          love => 92\n",
      "       machine => 88\n",
      "       mammals => 164\n",
      "          many => 46\n",
      "            me => 168\n",
      "     milkshake => 24\n",
      "         movie => 133\n",
      "          much => 29\n",
      "            my => 87\n",
      "         never => 97\n",
      "           not => 15\n",
      "           now => 26\n",
      "       numbers => 70\n",
      "            of => 57\n",
      "            on => 86\n",
      "       outside => 95\n",
      "       pancake => 65\n",
      "          ph.d => 51\n",
      "           pie => 34\n",
      "        played => 131\n",
      "        pomona => 141\n",
      "      protocol => 146\n",
      "          race => 115\n",
      "           ran => 114\n",
      "          read =>  5\n",
      "       reading => 144\n",
      "      research => 43\n",
      "         right => 25\n",
      "      semester => 58\n",
      "      sentence => 148\n",
      "         shark => 167\n",
      "           she => 52\n",
      "          shoe => 102\n",
      "        should => 149\n",
      "      sleeping => 10\n",
      "        sleeps => 162\n",
      "         snake => 129\n",
      "        sounds => 36\n",
      "          span => 45\n",
      "         sweet => 75\n",
      "         swims => 154\n",
      "         syrup => 63\n",
      "       teacher => 158\n",
      "         teeth => 69\n",
      "          test => 105\n",
      "          that =>  6\n",
      "           the =>  0\n",
      "         these => 68\n",
      "          they => 61\n",
      "         thing => 81\n",
      "          this => 41\n",
      "            to => 138\n",
      "          toby => 117\n",
      "         today => 159\n",
      "           too => 28\n",
      "         tools => 152\n",
      "         trapp =>  8\n",
      "          tree => 111\n",
      "         trees => 157\n",
      "       turtles => 166\n",
      "           use => 77\n",
      "      valuable => 80\n",
      "        walked => 94\n",
      "          wall => 119\n",
      "          want => 22\n",
      "       watched => 132\n",
      "    watermelon => 19\n",
      "            we => 89\n",
      "          went => 137\n",
      "         whale => 153\n",
      "        whales => 163\n",
      "        wizard => 100\n",
      "           won => 107\n",
      "         won't => 32\n",
      "    wonderland => 145\n",
      "         words => 90\n",
      "          work => 33\n",
      "         works => 85\n",
      "         wrote => 62\n",
      "          yerb => 128\n",
      "     yesterday => 72\n",
      "           you => 96\n",
      "          zayn => 120\n",
      "          zoom => 31\n",
      "\n",
      "Total number of words: 299\n",
      "Number of unique words: 169\n"
     ]
    }
   ],
   "source": [
    "print(\"       Vocabulary Indices\")\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "for word in sorted(word_to_index):\n",
    "    print(f\"{word:>14} => {word_to_index[word]:>2}\")\n",
    "\n",
    "print(\"\\nTotal number of words:\", total_words)\n",
    "print(\"Number of unique words:\", len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68e7bdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Indices\n",
      "-----------\n",
      "  D => 0\n",
      "  N => 1\n",
      "  V => 2\n",
      "\n",
      "Total number of tags: 299\n",
      "Number of unique tags: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Tag Indices\")\n",
    "print(\"-----------\")\n",
    "\n",
    "for tag, index in tag_to_index.items():\n",
    "    print(f\"  {tag} => {index}\")\n",
    "\n",
    "print(\"\\nTotal number of tags:\", total_tags)\n",
    "print(\"Number of unique tags:\", len(tag_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0753027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_words_to_index_tensor(words, mapping):\n",
    "    indices = [mapping[w] for w in words]\n",
    "    return torch.tensor(indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12178f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]),\n",
       " tensor([[ 1.3938,  0.4583, -0.7672, -1.2876,  1.1449,  1.3219],\n",
       "         [-1.1509,  0.3885, -0.4995, -0.8855,  1.0855,  0.5448]],\n",
       "        grad_fn=<EmbeddingBackward>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(word_to_index)  # Depends on the dataset\n",
    "embed_dim = 6  # Hyperparameter\n",
    "\n",
    "example_sentence = [\"dog\", \"ate\"]\n",
    "example_sentence_indices = convert_words_to_index_tensor(example_sentence, word_to_index)\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "embeddings = embedding_layer(example_sentence_indices)\n",
    "\n",
    "embeddings.shape, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c7e71b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 4]),\n",
       " tensor([[[-0.0900, -0.0537,  0.0928, -0.0771],\n",
       "          [-0.1454, -0.0688,  0.1296, -0.1164]]], grad_fn=<TransposeBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim = 4  # Hyperparamter\n",
    "num_layers = 4 # Hyperparameter\n",
    "\n",
    "lstm_layer = torch.nn.LSTM(embed_dim, state_dim, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "# We can ignore the hidden and cell state outputs\n",
    "lstm_output, (h_T, C_T) = lstm_layer(embeddings.unsqueeze(0))\n",
    "lstm_output.shape, lstm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "943c7c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3]),\n",
       " tensor([[[-0.4539, -0.2122, -0.5056],\n",
       "          [-0.4318, -0.2250, -0.5311]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(state_dim, tag_size)\n",
    "linear_output = linear_layer(lstm_output)\n",
    "linear_output.shape, linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5c13e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_LSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, state_dim, num_layers, tag_size):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = torch.nn.LSTM(embed_dim, state_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(state_dim, tag_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.embed(X)\n",
    "        X, _ = self.lstm(X)\n",
    "        return self.linear(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4a275fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "embed_dim = 6\n",
    "state_dim = 4\n",
    "num_layers = 2\n",
    "tag_size = len(tag_to_index)\n",
    "\n",
    "model = NN_LSTM(vocab_size, embed_dim, state_dim, num_layers, tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ddc7becf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3]),\n",
       " tensor([[[ 0.0020, -0.0391,  0.1059],\n",
       "          [ 0.0156, -0.0201,  0.1081]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = convert_words_to_index_tensor(example_sentence, word_to_index)\n",
    "yhat = model(x.unsqueeze(0))\n",
    "\n",
    "yhat.shape, yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f61e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_dim = 6\n",
    "state_dim = 4\n",
    "num_layers = 2\n",
    "\n",
    "valid_percent = 0.2\n",
    "learning_rate = 0.1\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "83a86d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 58)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset values\n",
    "N = len(dataset)\n",
    "vocab_size = len(word_to_index)\n",
    "tag_size = len(tag_to_index)\n",
    "\n",
    "# Shuffle the data so that we can split the dataset randomly\n",
    "shuffle(dataset)\n",
    "\n",
    "split_point = int(N * valid_percent)\n",
    "valid_dataset = dataset[:split_point]\n",
    "train_dataset = dataset[split_point:]\n",
    "\n",
    "len(valid_dataset), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7d4d2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(dataset, word_map, tag_map):\n",
    "    \"\"\"A helper function for computing accuracy on the given dataset.\"\"\"\n",
    "    total_words = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in dataset:\n",
    "            sentence_indices = convert_words_to_index_tensor(sentence, word_map)\n",
    "            tag_scores = model(sentence_indices.unsqueeze(0))\n",
    "            predictions = tag_scores.argmax(dim=1)\n",
    "            total_words += len(sentence)\n",
    "            print('tags', tags, predictions.shape)\n",
    "            print([p.item() for p in predictions.squeeze()])\n",
    "            total_correct += sum(t == tag_list[p.item()] for t, p in zip(tags, predictions.squeeze()))\n",
    "\n",
    "    return total_correct / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dac2d211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags ['N', 'V', 'N'] torch.Size([1, 3])\n",
      "[2, 1, 2]\n",
      "tags ['N', 'V', 'D', 'N'] torch.Size([1, 3])\n",
      "[3, 3, 3]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_876254/207582568.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation accuracy before training : {accuracy * 100:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_876254/3945815250.py\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[0;34m(dataset, word_map, tag_map)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tags'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtag_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_correct\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_876254/3945815250.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tags'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtag_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_correct\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "accuracy = compute_accuracy(valid_dataset, word_to_index, tag_to_index)\n",
    "print(f\"Validation accuracy before training : {accuracy * 100:.2f}%\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the data for each epoch (stochastic gradient descent)\n",
    "    shuffle(train_dataset)\n",
    "    \n",
    "    for sentence, tags in train_dataset:\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        sentence = convert_to_indices_tensor(sentence, word_indices)\n",
    "        tags = convert_to_indices_tensor(tags, tag_indices)\n",
    "        \n",
    "        tag_scores = model(sentence)\n",
    "        \n",
    "        loss = criterion(tag_scores, tags)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "accuracy = compute_accuracy(valid_dataset)\n",
    "print(f\"Validation accuracy after training  : {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce815d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013bb8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
