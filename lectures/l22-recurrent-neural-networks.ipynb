{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86b2051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (\"The dog ate the apple\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Everybody read that book\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Trapp is sleeping\".lower().split(), [\"N\", \"V\", \"V\"]),\n",
    "    (\"Everybody ate the apple\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Cats are good\".lower().split(), [\"N\", \"V\", \"D\"]),\n",
    "    (\"Dogs are not as good as cats\".lower().split(),[\"N\", \"V\", \"D\", \"D\", \"D\", \"D\", \"N\"]),\n",
    "    (\"Dogs eat dog food\".lower().split(), [\"N\", \"V\", \"N\", \"N\"]),\n",
    "    (\"Watermelon is the best food\".lower().split(), [\"N\", \"V\", \"D\", \"D\", \"N\"]),\n",
    "    (\"I want a milkshake right now\".lower().split(), [\"N\", \"V\", \"D\", \"N\", \"D\", \"D\"]),\n",
    "    (\"I have too much homework\".lower().split(), [\"N\", \"V\", \"D\", \"D\", \"N\"]),\n",
    "    (\"Zoom won't work\".lower().split(), [\"N\", \"D\", \"V\"]),\n",
    "    (\"Pie also sounds good\".lower().split(), [\"N\", \"D\", \"V\", \"D\"]),\n",
    "    (\"The college is having the department fair this Friday\".lower().split(),[\"D\", \"N\", \"V\", \"V\", \"D\", \"N\", \"N\", \"D\", \"N\"]),\n",
    "    (\"Research interests span many areas\".lower().split(), [\"N\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Alex is finishing his Ph.D\".lower().split(), [\"N\", \"V\", \"V\", \"D\", \"N\"]),\n",
    "    (\"She is the author\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"It is almost the end of the semester\".lower().split(), [\"N\", \"V\", \"D\", \"D\", \"N\", \"D\", \"D\", \"N\"]),\n",
    "    (\"Blue is a color\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"They wrote a book\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The syrup covers the pancake\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Harrison has these teeth\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The numbers are fractions\".lower().split(), [\"D\", \"N\", \"V\", \"N\"]),\n",
    "    (\"Yesterday happened\".lower().split(), [\"N\", \"V\"]),\n",
    "    (\"Caramel is sweet\".lower().split(), [\"N\", \"V\", \"D\"]),\n",
    "    (\"Computers use electricity\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"Gold is a valuable thing\".lower().split(), [\"N\", \"V\", \"D\", \"D\", \"N\"]),\n",
    "    (\"This extension cord helps\".lower().split(), [\"D\", \"D\", \"N\", \"V\"]),\n",
    "    (\"It works on my machine\".lower().split(), [\"N\", \"V\", \"D\", \"D\", \"N\"]),\n",
    "    (\"We have the words\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Trapp is a dog\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"This is a computer\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I love lamps\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"I walked outside\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"You never bike home\".lower().split(), [\"N\", \"D\", \"V\", \"N\"]),\n",
    "    (\"You are a wizard Harry\".lower().split(), [\"N\", \"V\", \"D\", \"N\", \"N\"]),\n",
    "    (\"Trapp ate the shoe\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Jett failed his test\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Alice won the game\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The class lasted a semester\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The tree had a branch\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I ran a race\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The dog barked\".lower().split(), [\"D\", \"N\", \"V\"]),\n",
    "    (\"Toby hit the wall\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Zayn ate an apple\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The cat fought the dog\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I got an A\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The A hurt\".lower().split(), [\"D\", \"N\", \"V\"]),\n",
    "    (\"I jump\".lower().split(), [\"N\", \"V\"]),\n",
    "    (\"I drank a yerb\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The snake ate a fruit\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I played the game\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I watched a movie\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Clark fixed the audio\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I went to Frary\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I go to Pomona\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Food are friends not fish\".lower().split(), [\"N\", \"V\", \"N\", \"D\", \"N\"]),\n",
    "    (\"You are reading this\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"Wonderland protocol is amazing\".lower().split(), [\"D\", \"N\", \"V\", \"D\"]),\n",
    "    (\"This is a sentence\".lower().split(), [\"D\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I should be doing homework\".lower().split(), [\"N\", \"V\", \"V\", \"V\", \"N\"]),\n",
    "    (\"Computers are tools\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"The whale swims\".lower().split(), [\"D\", \"N\", \"V\"]),\n",
    "    (\"A cup is filled\".lower().split(), [\"D\", \"N\", \"V\", \"V\"]),\n",
    "    (\"This is a cat\".lower().split(), [\"D\", \"V\", \"D\", \"N\"]),\n",
    "    (\"These are trees\".lower().split(), [\"D\", \"V\", \"N\"]),\n",
    "    (\"The cat is the teacher\".lower().split(), [\"D\", \"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"I ate food today\".lower().split(), [\"N\", \"V\", \"N\", \"N\"]),\n",
    "    (\"I am a human\".lower().split(), [\"N\", \"V\", \"D\", \"N\"]),\n",
    "    (\"The cat sleeps\".lower().split(), [\"D\", \"N\", \"V\"]),\n",
    "    (\"Whales are mammals\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"I like turtles\".lower().split(), [\"N\", \"V\", \"N\"]),\n",
    "    (\"A shark ate me\".lower().split(), [\"D\", \"N\", \"V\", \"N\"]),\n",
    "    (\"There are mirrors\".lower().split(), [\"D\", \"V\", \"N\"]),\n",
    "    (\"The bus spins\".lower().split(), [\"D\", \"N\", \"V\"]),\n",
    "    (\"Computers are machines\".lower().split(), [\"N\", \"V\", \"N\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "699caa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63e114fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "tag_to_index = {}\n",
    "\n",
    "total_words = 0\n",
    "total_tags = 0\n",
    "\n",
    "tag_list = []\n",
    "\n",
    "for words, tags in dataset:\n",
    "    \n",
    "    assert len(words) == len(tags)\n",
    "    \n",
    "    total_words += len(words)\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "    \n",
    "    total_tags += len(tags)\n",
    "    \n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_index:\n",
    "            tag_to_index[tag] = len(tag_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a91d625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Vocabulary Indices\n",
      "-------------------------------\n",
      "             a => 23\n",
      "          alex => 48\n",
      "         alice => 106\n",
      "        almost => 55\n",
      "          also => 35\n",
      "            am => 160\n",
      "       amazing => 147\n",
      "            an => 121\n",
      "         apple =>  3\n",
      "           are => 12\n",
      "         areas => 47\n",
      "            as => 16\n",
      "           ate =>  2\n",
      "         audio => 136\n",
      "        author => 53\n",
      "        barked => 116\n",
      "            be => 150\n",
      "          best => 20\n",
      "          bike => 98\n",
      "          blue => 59\n",
      "          book =>  7\n",
      "        branch => 113\n",
      "           bus => 171\n",
      "       caramel => 74\n",
      "           cat => 122\n",
      "          cats => 11\n",
      "         clark => 134\n",
      "         class => 109\n",
      "       college => 37\n",
      "         color => 60\n",
      "      computer => 91\n",
      "     computers => 76\n",
      "          cord => 83\n",
      "        covers => 64\n",
      "           cup => 155\n",
      "    department => 39\n",
      "           dog =>  1\n",
      "          dogs => 14\n",
      "         doing => 151\n",
      "         drank => 127\n",
      "           eat => 17\n",
      "   electricity => 78\n",
      "           end => 56\n",
      "     everybody =>  4\n",
      "     extension => 82\n",
      "        failed => 104\n",
      "          fair => 40\n",
      "        filled => 156\n",
      "     finishing => 49\n",
      "          fish => 143\n",
      "         fixed => 135\n",
      "          food => 18\n",
      "        fought => 123\n",
      "     fractions => 71\n",
      "         frary => 139\n",
      "        friday => 42\n",
      "       friends => 142\n",
      "         fruit => 130\n",
      "          game => 108\n",
      "            go => 140\n",
      "          gold => 79\n",
      "          good => 13\n",
      "           got => 124\n",
      "           had => 112\n",
      "      happened => 73\n",
      "      harrison => 66\n",
      "         harry => 101\n",
      "           has => 67\n",
      "          have => 27\n",
      "        having => 38\n",
      "         helps => 84\n",
      "           his => 50\n",
      "           hit => 118\n",
      "          home => 99\n",
      "      homework => 30\n",
      "         human => 161\n",
      "          hurt => 125\n",
      "             i => 21\n",
      "     interests => 44\n",
      "            is =>  9\n",
      "            it => 54\n",
      "          jett => 103\n",
      "          jump => 126\n",
      "         lamps => 93\n",
      "        lasted => 110\n",
      "          like => 165\n",
      "          love => 92\n",
      "       machine => 88\n",
      "      machines => 173\n",
      "       mammals => 164\n",
      "          many => 46\n",
      "            me => 168\n",
      "     milkshake => 24\n",
      "       mirrors => 170\n",
      "         movie => 133\n",
      "          much => 29\n",
      "            my => 87\n",
      "         never => 97\n",
      "           not => 15\n",
      "           now => 26\n",
      "       numbers => 70\n",
      "            of => 57\n",
      "            on => 86\n",
      "       outside => 95\n",
      "       pancake => 65\n",
      "          ph.d => 51\n",
      "           pie => 34\n",
      "        played => 131\n",
      "        pomona => 141\n",
      "      protocol => 146\n",
      "          race => 115\n",
      "           ran => 114\n",
      "          read =>  5\n",
      "       reading => 144\n",
      "      research => 43\n",
      "         right => 25\n",
      "      semester => 58\n",
      "      sentence => 148\n",
      "         shark => 167\n",
      "           she => 52\n",
      "          shoe => 102\n",
      "        should => 149\n",
      "      sleeping => 10\n",
      "        sleeps => 162\n",
      "         snake => 129\n",
      "        sounds => 36\n",
      "          span => 45\n",
      "         spins => 172\n",
      "         sweet => 75\n",
      "         swims => 154\n",
      "         syrup => 63\n",
      "       teacher => 158\n",
      "         teeth => 69\n",
      "          test => 105\n",
      "          that =>  6\n",
      "           the =>  0\n",
      "         there => 169\n",
      "         these => 68\n",
      "          they => 61\n",
      "         thing => 81\n",
      "          this => 41\n",
      "            to => 138\n",
      "          toby => 117\n",
      "         today => 159\n",
      "           too => 28\n",
      "         tools => 152\n",
      "         trapp =>  8\n",
      "          tree => 111\n",
      "         trees => 157\n",
      "       turtles => 166\n",
      "           use => 77\n",
      "      valuable => 80\n",
      "        walked => 94\n",
      "          wall => 119\n",
      "          want => 22\n",
      "       watched => 132\n",
      "    watermelon => 19\n",
      "            we => 89\n",
      "          went => 137\n",
      "         whale => 153\n",
      "        whales => 163\n",
      "        wizard => 100\n",
      "           won => 107\n",
      "         won't => 32\n",
      "    wonderland => 145\n",
      "         words => 90\n",
      "          work => 33\n",
      "         works => 85\n",
      "         wrote => 62\n",
      "          yerb => 128\n",
      "     yesterday => 72\n",
      "           you => 96\n",
      "          zayn => 120\n",
      "          zoom => 31\n",
      "\n",
      "Total number of words: 308\n",
      "Number of unique words: 174\n"
     ]
    }
   ],
   "source": [
    "print(\"       Vocabulary Indices\")\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "for word in sorted(word_to_index):\n",
    "    print(f\"{word:>14} => {word_to_index[word]:>2}\")\n",
    "\n",
    "print(\"\\nTotal number of words:\", total_words)\n",
    "print(\"Number of unique words:\", len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8220a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Indices\n",
      "-----------\n",
      "  D => 0\n",
      "  N => 1\n",
      "  V => 2\n",
      "\n",
      "Total number of tags: 308\n",
      "Number of unique tags: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Tag Indices\")\n",
    "print(\"-----------\")\n",
    "\n",
    "for tag, index in tag_to_index.items():\n",
    "    print(f\"  {tag} => {index}\")\n",
    "\n",
    "print(\"\\nTotal number of tags:\", total_tags)\n",
    "print(\"Number of unique tags:\", len(tag_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71a695b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_words_to_index_tensor(words, mapping):\n",
    "    indices = [mapping[w] for w in words]\n",
    "    return torch.tensor(indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d20c8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "embed_dim = 6 # Hyperparameter\n",
    "embed_layer = torch.nn.Embedding(vocab_size, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24739098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 6]),\n",
       " tensor([[ 1.9823, -1.7323,  0.3668, -1.7455, -0.3420,  1.3482],\n",
       "         [-0.5125, -0.9508, -0.3973, -0.3678,  0.5798, -0.4140],\n",
       "         [ 1.2710,  0.2771, -0.1242, -0.3522, -1.1643,  0.3411],\n",
       "         [ 1.9823, -1.7323,  0.3668, -1.7455, -0.3420,  1.3482],\n",
       "         [ 0.4866, -0.8927, -0.8811, -1.3986,  1.1610,  0.7400]],\n",
       "        grad_fn=<EmbeddingBackward>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i = torch.tensor([word_to_index[\"the\"], word_to_index[\"dog\"]])\n",
    "indices = convert_words_to_index_tensor(\"The dog ate the apple\".lower().split(), word_to_index)\n",
    "embed_output = embed_layer(indices)\n",
    "embed_output.shape, embed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4284a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 10\n",
    "lstm_layer = torch.nn.LSTM(embed_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "486bb0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 10])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (L, N, E)\n",
    "lstm_output, _ = lstm_layer(embed_output.unsqueeze(1))\n",
    "lstm_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75440441",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = torch.nn.Linear(hidden_dim, len(tag_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aefce339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1, 3]),\n",
       " tensor([[[-0.1569, -0.0117, -0.1078]],\n",
       " \n",
       "         [[-0.1784, -0.0688, -0.1352]],\n",
       " \n",
       "         [[-0.2861, -0.0483, -0.1489]],\n",
       " \n",
       "         [[-0.1702,  0.0042, -0.0940]],\n",
       " \n",
       "         [[-0.1342, -0.0075, -0.0763]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_output = linear_layer(lstm_output)\n",
    "linear_output.shape, linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f3293433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_LSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, tag_size):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, tag_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.embed(X)\n",
    "        X, _ = self.lstm(X.unsqueeze(1))\n",
    "        return self.linear(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b818e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = POS_LSTM(vocab_size, embed_dim, hidden_dim, len(tag_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dd5135d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34b33bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([5, 6])\n",
      "torch.Size([5, 1, 6])\n",
      "torch.Size([5, 1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0752, -0.2781, -0.0217]],\n",
       "\n",
       "        [[-0.0524, -0.2907,  0.0225]],\n",
       "\n",
       "        [[-0.0524, -0.2682,  0.0389]],\n",
       "\n",
       "        [[-0.0578, -0.3125,  0.0060]],\n",
       "\n",
       "        [[-0.0263, -0.3917, -0.0846]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e00cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
